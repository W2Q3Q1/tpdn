#!/bin/bash
#SBATCH --job-name=MyJob
#SBATCH --time=12:0:0
#SBATCH --partition=gpuk80
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=2
#SBATCH --mail-type=end
#SBATCH --mail-user=rmccoy20@jhu.edu
#SBATCH --output=train_model_ltr_auto.log
#SBATCH --error=train_model_ltr_auto.err
#SBATCH --exclude=gpu008,gpu019

module load pytorch
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first10 --prefix_prefix f10
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first50 --prefix_prefix f50
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first100 --prefix_prefix f100
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first500 --prefix_prefix f500
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first1000 --prefix_prefix f1000
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first5000 --prefix_prefix f5000
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first10000 --prefix_prefix f10000
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first20000 --prefix_prefix f20000
python model_trainer.py --encoder tpr --decoder ltr --task auto --enc_role_scheme wickel --prefix digits_first30000 --prefix_prefix f30000

